{"version":"1","records":[{"hierarchy":{"lvl1":"Saharan Dust Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Saharan Dust Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nThis Project Pythia Cookbook explores the relationship between Sharan dust aerosols (Bodèlè source) and meteorological variables using Self-Organizing Maps (SOM), Principal Component Analysis (PCA), and Random Forest Regression (RF).","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl2":"Motivation"},"content":"Several meteorological and land surface factors have been found to impact Saharan dust emissions and transport. Dust mobilization occurs when surface wind speed of suitable magnitude is above a threshold velocity, often computed based on soil characteristics, vegetation, and solid particles. The purpose of this cookbook is to understand the relationship between dust and some meteorological variables using self-organizing maps (\n\nSOM), random forest regression (\n\nRF), and principal component analysis (PCA). The question we are really interested in with this cookbook is to know which variables are the most predictive of dust emissions.\n\nFor experimenting purpose, we consider PM10 concentration (PM10), 2m temperature (T2), 2m relative humidity (rh2), planetary boundary layer height (PBLH), 10m wind speed (wind_speed_10m), 925hPa wind speed (wind_speed_925hPa), horizontal wind at 10m (U10), meridional wind at 10m (V10), and convective rainfall (RAINC). This cookbook seeks to establish a clear relationship between the meteorological variables and dust (PM10).","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl2":"Authors"},"content":"Jacob Tindan, \n\nPooja Hari Ambrish, \n\nAltug Karakurt, \n\nAli Fallah Acknowledge primary content authors here_","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl2":"Structure"},"content":"(State one or more sections that will comprise the notebook. E.g., This cookbook is broken up into two main sections - “Foundations” and “Example Workflows.” Then, describe each section below.)","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl3":"Section 1 ( Replace with the title of this section, e.g. “Foundations” )","lvl2":"Structure"},"type":"lvl3","url":"/#section-1-replace-with-the-title-of-this-section-e-g-foundations","position":10},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl3":"Section 1 ( Replace with the title of this section, e.g. “Foundations” )","lvl2":"Structure"},"content":"(Add content for this section, e.g., \"The foundational content includes ... \")","type":"content","url":"/#section-1-replace-with-the-title-of-this-section-e-g-foundations","position":11},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl3":"Section 2 ( Replace with the title of this section, e.g. “Example workflows” )","lvl2":"Structure"},"type":"lvl3","url":"/#section-2-replace-with-the-title-of-this-section-e-g-example-workflows","position":12},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl3":"Section 2 ( Replace with the title of this section, e.g. “Example workflows” )","lvl2":"Structure"},"content":"(Add content for this section, e.g., \"Example workflows include ... \")","type":"content","url":"/#section-2-replace-with-the-title-of-this-section-e-g-example-workflows","position":13},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":14},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":15},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":16},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":17},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":18},{"hierarchy":{"lvl1":"Saharan Dust Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace “cookbook-example” with the title of your cookbooks)\n\nClone the https://github.com/ProjectPythia/cookbook-example repository: git clone https://github.com/ProjectPythia/cookbook-example.git\n\nMove into the cookbook-example directorycd cookbook-example\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate cookbook-example\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":19},{"hierarchy":{"lvl1":"Principal Component Analysis"},"type":"lvl1","url":"/notebooks/pca","position":0},{"hierarchy":{"lvl1":"Principal Component Analysis"},"content":"Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.The idea of PCA is to reduce the number of variables of a data set, while preserving as much information as possible.\n\nIn this cookbook we will be implementing PCA to reduce the number of variables in our data set by preserving 95% of the data’s variance\n\n# load the required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.preprocessing import scale\nfrom sklearn import preprocessing \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nThe first step is to read the data.\nPlease refer to the Random Forest Regression Cookbook for a detailed explanation about the feature engineering. Here we will be reading the dataset with the new variables obtained after feature engineering.\n\n\ndust_df = pd.read_csv('../saharan_dust_met_cat_vars.csv', index_col='time')\n\n\ndust_df\n\n","type":"content","url":"/notebooks/pca","position":1},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"One Hot Encoding"},"type":"lvl2","url":"/notebooks/pca#one-hot-encoding","position":2},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"One Hot Encoding"},"content":"\n\nLet’s look at the unique values of the WIND_DIR feature\n\ndust_df['WIND_DIR'].unique()\n\nSince WIND_DIR is a categorical variable, we have to perform One Hot Encoding(OHE). One Hot Encoding creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature. One Hot Encoding is the process of creating dummy variables.\n\none_hot = pd.get_dummies(dust_df['WIND_DIR'], dtype=int)\n\none_hot\n\ndf_new = dust_df.drop('WIND_DIR', axis=1)\n\nLet’s now join the One Hot Encoded Fatures into the original dataframe\n\ndf_new = df_new.join(one_hot)\n\ndf_new\n\nX_features = df_new.drop(labels='PM10', axis = 1)\n\nX_features\n\nWe will now separate out the features and the target variable\n\nX = df_new.drop(labels='PM10', axis=1).values\ny = df_new['PM10'].values\n\nX.shape\n\ny.shape\n\nHere we will be splitting the data into training and testing data\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n","type":"content","url":"/notebooks/pca#one-hot-encoding","position":3},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl3":"Feature Scaling","lvl2":"One Hot Encoding"},"type":"lvl3","url":"/notebooks/pca#feature-scaling","position":4},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl3":"Feature Scaling","lvl2":"One Hot Encoding"},"content":"\n\nFeature scaling is a vital pre-processing step in machine learning that involves transforming numerical features to a common scale. Scaling techniques aim to normalize the range, distribution, and magnitude of features, reducing potential biases and inconsistencies that may arise from variations in their values.\n\nThe next step is to scale the data. We will be using RobustScaler for scaling.\nRobustScaler is an algorithm that scales features using statistics that are robust to outliers.\n\nThis Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile) but can be configured.\n\nWhen applying feature scaling, it’s crucial to fit the scaler on the training data and then use the same scaler to transform the test data. This ensures consistency and prevents data leakage.\n\nscaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nWe will now use Scikit-Learn’s PCA estimator to find out the number of principal components we need. Since we are not sure how many principal components we need for this data, we will use Minka’s MLE to guess the dimension. Use of n_components == ‘mle’ will interpret svd_solver == ‘auto’ as svd_solver == ‘full’\n\npca = PCA(n_components = 'mle', svd_solver = 'full')\n\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\n\n#Print dataset shape before and after PCA\nprint(f'\\nTraining Data shape: {X_train.shape}')\nprint(f'PCA Training Data shape: {X_train_pca.shape}')\n\nFrom the Scikit-learn implementation, we can get the information about the explained variance and plot the cumulative variance.\nThe importance of each component is represented by the explained variance ratio, which indicates the portion of the variance that lies along each principal component.\n\nexplained_variance = pca.explained_variance_ratio_\n\nexplained_variance\n\nBy summing the explained variance ratio of the first N{pc} components, we obtain the so-called cumulative explained variance\n\ncumulative_var_ratio = np.cumsum(explained_variance)\n\ncumulative_var_ratio\n\nThe minimum number of principal components required to preserve the 95% of the data’s variance can be computed with the following command\n\nd = np.argmax(cumulative_var_ratio >= 0.95) + 1\nd\n\nLet’s plot the cumulative variance for the number of principal components\n\nplt.rcParams[\"figure.figsize\"] = (10,6)\nfig, ax = plt.subplots()\nxi = np.arange(0,10, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\nplt.xlim([0,10])\nplt.ylim([0.0,1.1])\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0,10, step=1)) \nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=10)\n\nax.grid(axis='x')\nplt.show()\n\nThis plot illustrates how the cumulative variance explained by principal components increases as more components are added. It helps determine the number of components needed to capture the majority of the variance in the dataset.\nHence we can conclude from the plot and from calculation that in order to preserve 95% of the data’s variance, we need 6 principal components.\n\nNow let’s run the PCA estimator with n_components set to 6\n\npca = PCA(n_components = 6, svd_solver = 'full')\n\nX_train_pca_6 = pca.fit_transform(X_train)\nX_test_pca_6 = pca.transform(X_test)\n\n\nprint(pca.components_)\n\n#Print dataset shape before and after PCA\nprint(f'\\nTraining Data shape: {X_train.shape}')\nprint(f'PCA Training Data shape: {X_train_pca_6.shape}')\n\n# number of components\nn_pcs= pca.components_.shape[0]\nn_pcs\n\nWe now know that out of 11 features, 6 features were selected by PCA as relevant features but we do not know what those 6 features are. We can do that by first finding the index of the feature with the largest absolute value in each principal component’s loading vector using list comprehension. Then we can map these indices to their corresponding feature names from the original dataset. The next step is to create a dictionary that associates each principal component with its most important feature and convert the dictionary to a dataframe\n\n# get the index of the most important feature on EACH component(largest absolute value)\n\nmost_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n\n#get the corresponding feature names from the original dataset\ninitial_feature_names = list(X_features.columns.values)\ninitial_feature_names\n\n# map the indices to their corresponding feature names from the original dataset\nmost_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n\n# create a dictionary that associates each principal component with its most important feature \ndic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n\n# build the dataframe\ndf = pd.DataFrame(sorted(dic.items()))\n\ndf\n\nNow we know the 6 principal components that were selected by the PCA estimator.\nThey are\n\nwind_speed_10m\n\nrh2\n\nPBLH\n\nRAIN\n\nslp\n\nNE\n\n","type":"content","url":"/notebooks/pca#feature-scaling","position":5},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"Let’s visualize our results"},"type":"lvl2","url":"/notebooks/pca#lets-visualize-our-results","position":6},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"Let’s visualize our results"},"content":"\n\nThe PCA-transformed data is clustered using KMeans with 6 clusters.\n\nK-means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into K clusters. It works by iteratively assigning each data point to the nearest centroid and then updating the centroids based on the mean of the data points assigned to each cluster.\n\n\n\n# Cluster the PCA-transformed data\nkmeans = KMeans(n_clusters=6, random_state=42)\ncluster_labels = kmeans.fit_predict(X_train_pca_6)\n\n# Create a DataFrame combining PCA components and cluster labels\npca_cluster_df = pd.DataFrame(X_train_pca_6, columns=[f'PC{i+1}' for i in range(6)])\npca_cluster_df['Cluster'] = cluster_labels\n\n# Get the feature names associated with each cluster\nn_pcs = pca.components_.shape[0]\nmost_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\ninitial_feature_names = list(X_features.columns.values)\nmost_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n\n# Plot the scatter plot\nplt.figure(figsize=(10, 8))\nsns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_cluster_df, palette='tab10')\nplt.title('PCA Components in Clusters')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\n\n# Create custom legend\nlegend_labels = {i: most_important_names[i] for i in range(len(most_important_names))}\nplt.legend(title='Feature', labels=legend_labels.values())\n\nplt.show()\n\n\nHere the PCA components and cluster labels are combined into a DataFrame. A scatter plot is created where each point represents a data sample, and the points are colored based on their cluster assignment.\n\nIn the upcoming notebook, we’ll delve into Self-Organizing Maps (SOM) and explore their application on this dataset. We will discover how SOMs are leveraged for clustering and visualization, offering insights into the underlying structures and patterns within the data.","type":"content","url":"/notebooks/pca#lets-visualize-our-results","position":7},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)"},"type":"lvl1","url":"/notebooks/som","position":0},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)"},"content":"# load python packages\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport math\nimport matplotlib as mpl\n\n# load machine learning packages\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn import preprocessing\n\n# minisom package for SOM\n!pip install minisom\nimport minisom\nfrom minisom import MiniSom\n\nfrom matplotlib.patches import Patch\n\nfrom matplotlib.patches import RegularPolygon, Ellipse\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom matplotlib import cm, colorbar\nfrom matplotlib.lines import Line2D\nimport matplotlib.cbook as cbook\n\n","type":"content","url":"/notebooks/som","position":1},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl3":"Global figure settings"},"type":"lvl3","url":"/notebooks/som#global-figure-settings","position":2},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl3":"Global figure settings"},"content":"\n\n# customize figure \nmpl.rcParams['font.size'] = 18\nmpl.rcParams['legend.fontsize'] = 'large'\nmpl.rcParams['figure.titlesize'] = 'large'\nmpl.rcParams['lines.linewidth'] = 2.5\nmpl.rcParams['axes.linewidth'] = 2.5\nmpl.rcParams[\"axes.unicode_minus\"] = True\nmpl.rcParams['figure.dpi'] = 150\nmpl.rcParams['savefig.bbox']='tight'\nmpl.rcParams['hatch.linewidth'] = 2.5\n\n# little function\ndef remove_time_mean(x):\n    return x - x.mean(dim='time')\n\n","type":"content","url":"/notebooks/som#global-figure-settings","position":3},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl2":"Importing data from disk"},"type":"lvl2","url":"/notebooks/som#importing-data-from-disk","position":4},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl2":"Importing data from disk"},"content":"\n\ndust_df = pd.read_csv('../saharan_dust_met_vars.csv', index_col='time')\n\n# print out shape of data \nprint('Shape of data:', np.shape(dust_df))\n\n# print first 5 rows of data\nprint(dust_df.head())\nfeature_names = dust_df.columns\n\n","type":"content","url":"/notebooks/som#importing-data-from-disk","position":5},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl3":"Scaling the variables using the robust scaling method","lvl2":"Importing data from disk"},"type":"lvl3","url":"/notebooks/som#scaling-the-variables-using-the-robust-scaling-method","position":6},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl3":"Scaling the variables using the robust scaling method","lvl2":"Importing data from disk"},"content":"\n\n# normalization by minmax scaling \nminmax_sc = preprocessing.MinMaxScaler(feature_range = (0, 1))\nminmax_sc.fit(dust_df)\nminmax_scaled_df = minmax_sc.transform(dust_df)\n\n# standardization\nstd_sc = preprocessing.StandardScaler().fit(dust_df)\nstd_scaled_df = std_sc.transform(dust_df)\n\n# Robust scaling for outliers\nrb_sc = preprocessing.RobustScaler().fit(dust_df)\nrob_scaled_df = rb_sc.transform(dust_df)\n\n","type":"content","url":"/notebooks/som#scaling-the-variables-using-the-robust-scaling-method","position":7},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl2":"Define map and train the SOM map"},"type":"lvl2","url":"/notebooks/som#define-map-and-train-the-som-map","position":8},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl2":"Define map and train the SOM map"},"content":"\n\nThe authors of SOM recommend a map size of 5\\times \\sqrt{(number\\_of\\_samples)}\n\n See a full documentation of the \n\nSOM package\n\n# training data\ntrain_data = rob_scaled_df\n# Define minisom model\nn_samples = train_data.shape[1]  # retrieves the number of sampe (10 for this cookbook) \nnum = math.ceil(5*math.sqrt(n_samples))\nsom = MiniSom(x=num,\n              y=num, # map size, NxN\n              input_len=10, # number of features used for training the model (10 element input vectors)\n              sigma=3.,     # sigma: is the radius of the different neighbors in the SOM\n              learning_rate=0.5, # learning rate: determines how much the weights are adjusted during each iterations\n              neighborhood_function='gaussian', # a few options for this\n             topology='hexagonal', \n              activation_distance='euclidean', \n              random_seed=10)\n\n# initilize weight\nsom.random_weights_init(train_data)  # random weights initialization \n#som.pca_weights_init(train_data)  # initialize weights using PCA \n\n## there are two type of training\n# 1. train_random: trains model by pickinhg random data from the data\n# 2. train_batch: trains model from samples in the order in which they are fed.\n\nsom.train(data = train_data, num_iteration = 25000, \n          verbose=True, random_order=True) # training the SOM model for 25000 iterations \n\n","type":"content","url":"/notebooks/som#define-map-and-train-the-som-map","position":9},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl2":"Visualizing the SOM results"},"type":"lvl2","url":"/notebooks/som#visualizing-the-som-results","position":10},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl2":"Visualizing the SOM results"},"content":"\n\n","type":"content","url":"/notebooks/som#visualizing-the-som-results","position":11},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl3":"Neighbour distance","lvl2":"Visualizing the SOM results"},"type":"lvl3","url":"/notebooks/som#neighbour-distance","position":12},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl3":"Neighbour distance","lvl2":"Visualizing the SOM results"},"content":"\n\nThe neighbor distance is also known as the “U-Matrix”. It is the distance between each node and its neighbours. Regions of low neighbourhood distance indicate groups of nodes that are similar, while regions of large distances indicate nodes are much more different. The U-Matrix can be used to identify clusters within the SOM map.\n\nfrom pylab import bone, pcolor, colorbar, plot, show\nbone()\npcolor(som.distance_map().T, cmap='bone')\ncolorbar()\n\n","type":"content","url":"/notebooks/som#neighbour-distance","position":13},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl3":"Feature plane","lvl2":"Visualizing the SOM results"},"type":"lvl3","url":"/notebooks/som#feature-plane","position":14},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl3":"Feature plane","lvl2":"Visualizing the SOM results"},"content":"This is a map for each variable used, and it shows the magnitude of the weights associated to it for each neuron.\n\ntitles = ['PM10', '2m temperature', '2m relative humidity', 'Mean sea level pressure', 'Boundary layer depth', \n          'Convective rainfall', '10m wind speed', '925hPa wind speed', 'horizontal wind', 'meridional wind']\nW = som.get_weights()\nfig, ax = plt.subplots(3, 4, figsize=(16, 10))\nax = ax.flatten()\nfor i, f in enumerate(feature_names):\n    #plt.subplot(5, 2, i+1)\n    ax[i].set_title(titles[i])\n    ax[i].pcolor(W[:,:,i].T, cmap='coolwarm')\n    #ax[i].set_xticks(np.arange(num+1))\n    #ax[i].set_yticks(np.arange(num+1))\nax[10].set_axis_off()\nax[11].set_axis_off()\nplt.tight_layout()\nplt.savefig('feature_patterns.png')\n\nWe see a pattern which establishes the relationship between each of the variables under consideration.\nFor example, nodes of higher PM10 concentration are associated with nodes of higher 10m wind speed and 925hPa wind speed and vice versa. The horizontal and meridional wind patterns suggest a prevailing northeasterly trade winds, which are known to be associated with dust emissions and transport. Regions of high PM10 concentration correspond to somewhat lower temperatures, which may be expected, but more diurnal scale analysis is need to verify this statement. The analysis also reveals that increased dust concentration may lower relative humidity at the surface, and this could potentially affect tropical cyclone development over the North Atlantic as been speculated in many papers.\n\n","type":"content","url":"/notebooks/som#feature-plane","position":15},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl3":"Most relevant feature plane","lvl2":"Visualizing the SOM results"},"type":"lvl3","url":"/notebooks/som#most-relevant-feature-plane","position":16},{"hierarchy":{"lvl1":"Self-organizing Maps (SOM)","lvl3":"Most relevant feature plane","lvl2":"Visualizing the SOM results"},"content":"By associating each neuron to the feature with the maximum weight, we divide the map into distinct regions, where specific features are most important (high values).\n\nZ = np.zeros((num, num))\nW = som.get_weights()\nplt.figure(figsize=(16, 10))\nfor i in np.arange(som._weights.shape[0]):\n    for j in np.arange(som._weights.shape[1]):\n        feature = np.argmax(W[i, j , :])\n        plt.plot([i+.5], [j+.5], color='C'+str(feature),\n                 marker='o', markersize=24)\n\nlegend_elements = [Patch(facecolor='C'+str(i),\n                         edgecolor='w',\n                         label=f) for i, f in enumerate(feature_names)]\n\nplt.legend(handles=legend_elements,\n           loc='center left',\n           bbox_to_anchor=(1, .7))     \nplt.xlim([0, num])\nplt.ylim([0, num])\nplt.show()","type":"content","url":"/notebooks/som#most-relevant-feature-plane","position":17},{"hierarchy":{"lvl1":"Exploratory Data Analysis"},"type":"lvl1","url":"/notebooks/data-exploration","position":0},{"hierarchy":{"lvl1":"Exploratory Data Analysis"},"content":"In this exercise, we are using machine learning as a tool for data exploration. We are interested in discovering the impact of the other attributes on PM10 dust concentration. One way to investigate this is to check the contribution of each attribute to the prediction accuracy of a learning algorithm. Our algorithm of chocie is \n\nrandom forests due to the interpretability of the resultant predictor.\n\n# Importing relevant libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\n","type":"content","url":"/notebooks/data-exploration","position":1},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl2":"Key packages"},"type":"lvl2","url":"/notebooks/data-exploration#key-packages","position":2},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl2":"Key packages"},"content":"package\n\nUse\n\nminisom package\n\nSelf-organizing maps (SOM)\n\nscikit-learn packages\n\nMachine learning\n\nmatplotlib\n\nfor plotting\n\n","type":"content","url":"/notebooks/data-exploration#key-packages","position":3},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl4":"Global figure settings","lvl2":"Key packages"},"type":"lvl4","url":"/notebooks/data-exploration#global-figure-settings","position":4},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl4":"Global figure settings","lvl2":"Key packages"},"content":"This piece of code sets the font size, line widths, figure title size, and resolution for all figures generated in this cookbook.\n\n# customize figure \nimport matplotlib as mpl\nmpl.rcParams['font.size'] = 28\nmpl.rcParams['legend.fontsize'] = 'large'\nmpl.rcParams['figure.titlesize'] = 'large'\nmpl.rcParams['lines.linewidth'] = 2.5\nmpl.rcParams['axes.linewidth'] = 2.5\nmpl.rcParams[\"axes.unicode_minus\"] = True\nmpl.rcParams['figure.dpi'] = 150\nmpl.rcParams['savefig.bbox']='tight'\nmpl.rcParams['hatch.linewidth'] = 2.5\n\n","type":"content","url":"/notebooks/data-exploration#global-figure-settings","position":5},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl2":"Import data"},"type":"lvl2","url":"/notebooks/data-exploration#import-data","position":6},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl2":"Import data"},"content":"\n\n# Loading and visualizing the data\ndust_df = pd.read_csv('../saharan_dust_met_vars.csv', index_col='time')\n# print out shape of data \nprint('Shape of data:', np.shape(dust_df))\n\n# print first 5 rows of data\nprint(dust_df.head())\n\n","type":"content","url":"/notebooks/data-exploration#import-data","position":7},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl3":"What is the data distribution?","lvl2":"Import data"},"type":"lvl3","url":"/notebooks/data-exploration#what-is-the-data-distribution","position":8},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl3":"What is the data distribution?","lvl2":"Import data"},"content":"Understand your data first before usage.\n\nplt.rcParams['figure.figsize'] = [22, 16]\ndust_df.hist(bins=25);\nplt.tight_layout()\n\nThe histograms above are showing us the how the attributes vary across the samples in the dataset. The histogram that stands out the most is RAINC due to its strong concentration around a small section of its range. The long tail of the histogram extends to 700 to capture some rare occurences of heavy rain, but most days show little to no precipitation, falling under 100. In order to capture these rare events, the majority of the samples are compressed in a small range. To represent the typical days more accurately, we quantize RAINC to capture the rain events without the skewing effect of the concentrated distribution.\n\nUsing \n\nAMS definitions, and keeping in mind that our data captures a 24 hour range, we use the following conversion on the RAINC attribute:\n\n0        --> No rain (value 0)\n\n(0, 24]  --> Drizzle (value 1)\n\n(24,60]  --> Light rain (value 2)\n\n(60,182] --> Moderate rain (value 3)\n\n182+     --> Heavy rain (value 4)\n\n# Converting the RAINC attribute\ndef cat_precip(row):\n        if(row['RAINC'] == 0):\n            return 0 #'NR'\n        elif((row['RAINC'] > 0) and (row['RAINC'] <= 24)):\n            return 1 #'D'\n        elif((row['RAINC'] > 24) and (row['RAINC'] <= 60)):\n            return 2 #'LR'\n        elif((row['RAINC'] > 60) and (row['RAINC'] <= 182)):\n            return 3 #'MR'\n        elif(row['RAINC'] > 82):\n            return 4 #'HR'\n\ndust_df['RAIN'] = dust_df.apply(cat_precip, axis=1)\ndust_df = dust_df.drop(columns=[\"RAINC\"])\n\nAnother factor in feature engineering is the over-representation of the wind speed. In adition to the wind speed attributes at two elevations (wind_speed_10m, wind_speed_925hPa), we have the U10 and V10 variables implicitly encoding the same information. We reduce this over-emphasis by converting the latter two variables into a categorical attribute that only encodes the wind direction in one of the 4 possible values: Southeast (SE), Southwest (SW), Northeast (NE) and Northwest (NW). In our dataset there is no instance where either of the directions are zero, so we don’t need to represent the directions East, West, South and North.\n\nFor further applications that qould require further detail in wind direction, one might consider increasing the granularity by including more categories like North-Northwest, West-Northwest, etc. that captures which of the two primary directions the wind is closer to.\n\n# Converting U10 and V10 attributes\ndef cat_wind_dir(row):\n    if((row['U10']>=0) and (row['V10']>=0)):\n        return 'SW'\n    if((row['U10']>=0) and (row['V10']<0)):\n        return 'NW'\n    if((row['U10']<0) and (row['V10']>=0)):\n        return 'SE'\n    else:\n        return 'NE'\n\ndust_df['WIND_DIR'] = dust_df.apply(cat_wind_dir, axis=1)\ndust_df = dust_df.drop(columns=['U10','V10'])\ndust_df.head()\n\n","type":"content","url":"/notebooks/data-exploration#what-is-the-data-distribution","position":9},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl3":"NB: This dataframe of dust_df is saved on disk for future use.","lvl2":"Import data"},"type":"lvl3","url":"/notebooks/data-exploration#nb-this-dataframe-of-dust-df-is-saved-on-disk-for-future-use","position":10},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl3":"NB: This dataframe of dust_df is saved on disk for future use.","lvl2":"Import data"},"content":"\n\n","type":"content","url":"/notebooks/data-exploration#nb-this-dataframe-of-dust-df-is-saved-on-disk-for-future-use","position":11},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl3":"Are there any outliers?","lvl2":"Import data"},"type":"lvl3","url":"/notebooks/data-exploration#are-there-any-outliers","position":12},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl3":"Are there any outliers?","lvl2":"Import data"},"content":"Some machine learning models do not perform well when there are outliers in the data. This section explores the data for any potential outliers. The boxplots show there are outliers in the data, hence we need to use a scaling method which is robust on outliers.\n\ndust_df = pd.read_csv('../saharan_dust_met_vars.csv', index_col='time')\nfeature_names = dust_df.columns\n\nfig, ax = plt.subplots(3,4, figsize=(26,12))\nax = ax.flatten()\nfor ind, col in enumerate(feature_names):\n    ax[ind].boxplot(dust_df[col].dropna(axis=0), \n                    notch=True, whis=1.5,  \n                    showmeans=True)\n    ax[ind].grid(which='minor', axis='both')\n    ax[ind].set_xticklabels([''])\n    ax[ind].set_ylabel(col)\n    #ax[ind].set_title(col)\n    ax[ind].set_facecolor('white')\n    ax[ind].spines['bottom'].set_color('0.1')\n    ax[ind].spines['top'].set_color('0.1')\n    ax[ind].spines['right'].set_color('0.1')\n    ax[ind].spines['left'].set_color('0.1')\n    ax[ind].grid(True)\n    ax[10].set_axis_off()\n    ax[11].set_axis_off()\nfig.tight_layout()\n#plt.savefig('box_plots.png')\n\n","type":"content","url":"/notebooks/data-exploration#are-there-any-outliers","position":13},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl3":"Scaling the variables","lvl2":"Import data"},"type":"lvl3","url":"/notebooks/data-exploration#scaling-the-variables","position":14},{"hierarchy":{"lvl1":"Exploratory Data Analysis","lvl3":"Scaling the variables","lvl2":"Import data"},"content":"As you can see, there is a large range of values among variables. SOM and PCA are scale variant, so to not influence the results as it is the case in many unsupervised machine learning models, it is important to scale them. Many scaling methods exist, but we will use the robust scaling method since this takes care of outliers.\n\nThis concludes the primary exploration of our data. We now do a deeper dive into our exploration by using statistical analysis through Principal Component Analysis.","type":"content","url":"/notebooks/data-exploration#scaling-the-variables","position":15},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"Random Forest Regression"},"type":"lvl1","url":"/notebooks/random-forest","position":0},{"hierarchy":{"lvl1":"Random Forest Regression"},"content":"In this exercise, we are using machine learning as a tool for data exploration. We are interested in discovering the impact of the other features (attributes) on PM10 dust concentration. One way to investigate this is to implement a PM10 predictor that uses the other features as input and then check the contribution of each attribute to the learning algorithm. Our algorithm of chocie is \n\nrandom forests due to the interpretability of the outcome.\n\nRandom forest is an ensemble algorithm, a class of learning algorithms that rely on combining the predictions of many ‘weak’ learners instead of relying on a single ‘strong’ learner. In practice, this approach is used to avoid the common issue of overfitting, where a learning algorithm fits the given dataset too well. In short, an overfit algorithm goes beyond just learning the nature and statistical properties data, it learns the particular intricacies of the specific examples in the dataset. Hence, it deviates from learning the trends in the data and learning the irrelevant noise in it. In other words, an overfit algorithm loses sight of the forest for the trees. The aptly named random forest algorithm instead uses an ensemble of learners, called decision trees, that are trained to lean towards under-fitting rather than over-fitting and uses the consensus of these weak learners to come to a final decision.\n\nThe decision trees that make up the random forest abstract the idea of a flowchart, mimicking a natural way humans make decisions. The training of a decision tree starts at the root node and at every step asks the question, “What is the most distinctive feature and how does it distinguish the target variable?”. Based on the answer, the tree branches and keeps iteratively asking the same question at each branch for the subset of data that lands on the given side. Thinking back to out primary goal with this cookbook, the decision trees are internally asking the same questions that we do.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor \n\n# We are loading the edited version of the dataset we saved in the first section.\ndust_df = pd.read_csv('../saharan_dust_met_cat_vars.csv', index_col='time')\n\n# Similar to the PCA example, we need to one-hot encode the wind direction since random forests only support numerical features.\none_hot = pd.get_dummies(dust_df['WIND_DIR'])\ndust_df = dust_df.drop('WIND_DIR', axis=1)\ndust_df = dust_df.join(one_hot)\n\n# Separating our data into the target variable (PM10) and the features (the rest fo the attributes)\ny = dust_df['PM10'].values\nX = dust_df.drop(['PM10'], axis=1).values\n\nA key step in training learning algorithms is the hyperparameter tuning step. These parameters specify the predictor we are using and can be used as levers to fine tune how well the algorithm fits the given dataset. These chocies are critical in preventing under-fitting and over-fitting in most learning algorithms. However, due to our choice of a resilient ensemble algorithm, these choices end up being less impactful than other algorithms like neural networks or a stand-alone decision-tree. We include the best performing choices of the parameters in our attempts, but feel free to change these values to see how they imapct the rest of our analysis.\n\nBelow is a quick summary of how the most significant hyper-parameters impact the resultant random forest. For much more detail, you can find the in-depth \n\nuser guide.\n\nn_estimators: This is the number of decision-trees included in the random forest. The larger the number, the more robust we expect the algorithm to get to overfitting, which comes at a computational cost during training.\n\nmax_depth: The maximum depth for the decision-trees in the algorithm. The deeper the tree gets, the more likely it is to over-fit (and conversely, the shallower the tree, the more likely it is to under-fit). Using an ensemble of trees instead of a single one reduces the impact of this hyper-parameter.\n\nmax_features: The number of features each branch can make decisions on. The larger the number, the likelier a tree is to over-fit and vice versa. Similar to max_depth, using random forest makes it less crucial to tune this parameter well. We leave it at its default value.\n\nn_jobs: Enables training decision-trees in parallel. Our choices maximizes the parallelisim to complete training as fast as possible.\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=100, n_jobs=-1)\n\n# This step might take a few minutes. We slice the data into 5 random chunks.\n# Then, each chunk is spared for testing, the training is done on the other 4 and tested on the spared chunk.\n# This is repeated 5 times and the average of the result is reported. \nscores = cross_val_score(rf, X, y, cv=5, scoring=\"neg_mean_absolute_percentage_error\")\nprint(f\"The random forest predicts the dust level with {scores.mean()+1} average accuracy\")\n\nFor evaluation, we are using a relative metric. The reported loss value is the relative difference (w.r.t to the true value) between the true and the predicted value. So, the ~60% accuracy we get here means our prediction deviates from the true value by 40% on average. Although this figure looks large at first glance, notice that the target value varies across a large range that makes orders of magnitude mistakes possible.\n\ndust_df['PM10'].hist(bins=25);\n\nThis step is a typical part of the machine learning workflow for tuning hyper-parameters. Once we finalize the hyper-parameters, we can re-train the algorithm on all of our data (instead of 4/5 of it) and start exploring te importance of the features on our target variable.\n\n# Training a new random forest on our entire data\ny = dust_df['PM10'].values\nX = dust_df.drop(['PM10', 'wind_speed_10m'], axis=1).values\nfeatures = dust_df.drop(['PM10'], axis=1).columns\nrf = RandomForestRegressor(n_estimators=100, max_depth=100, n_jobs=-1)\nrf.fit(X, y);\n\nA very useful property of random forests is their interpretability. As useful as they are in practice, quite a lot of learning algorithms are hard to interpret once they are trained. Although there are exceptions, artificial neural networks are hard to make sense of despite their impressive performance, making them practical black-boxes. Random forests on the other hand, keep track of the importance of each feature and the ranges of each that dictate the target variable. We use this information to rank how our features impact the PM10 dust levels.\n\nfig, ax = plt.subplots(1,1, figsize=(22,14), sharex=False, sharey=False,\n                               constrained_layout=True)\nsorted_idx = rf.feature_importances_.argsort()\nimportances = rf.feature_importances_\n\n# Make a bar chart\ncols = np.array(['r','b','g','c','k','pink','purple','magenta','olive'])\nax.set_xlabel('Random Forest Feature Importance')\nax.set_ylabel('Features')\nax.set_axisbelow(True)\nax.grid()\nax.barh(features[sorted_idx],\n           importances[sorted_idx], color=cols);\n\nWe see in this figure the significant impact of the wind speed at 10 meters. However, this dominance overshadows the impact of the other featsures. To see how important each feature is, we use the accuracy of our predictor that has all the features as the baseline and re-train predictors by ;eaving one feature out at a time. Then, comparing the performance of these predictors with the benchmark can help us quantify how much our predictor’s performance changes by including each feature.\n\nbenchmark_accuracy = scores.mean()\nleave_one_out_accuracies = []\n\n# We exclude the one-hot encoded features\nleave_one_out_features = ['T2', 'rh2', 'slp', 'PBLH', 'wind_speed_10m', 'wind_speed_925hPa','RAIN']\n\nfor feat in leave_one_out_features:\n    dropped_labels = ['PM10']\n    dropped_labels.append(feat)\n    features = dust_df.drop(dropped_labels, axis=1)\n    X = dust_df.drop(dropped_labels, axis=1).values\n    rf = RandomForestRegressor(n_estimators=100, max_depth=100, n_jobs=-1);\n    current_scores = cross_val_score(rf, X, y, cv=5, scoring=\"neg_mean_absolute_percentage_error\")\n    leave_one_out_accuracies.append(current_scores.mean())\n    print(f\"The random forest predicts the dust level with {current_scores.mean()+1} accuracy when we leave {dropped_labels[1]} out\")\n\nfig, ax = plt.subplots(1,1, figsize=(22,14), sharex=False, sharey=False, constrained_layout=True);\nax.barh(leave_one_out_features, [benchmark_accuracy-loo for loo in leave_one_out_accuracies]);\nax.set_xlabel('Change in prediction accuracy when each feature is added to the dataset');\nax.set_ylabel('Features');","type":"content","url":"/notebooks/random-forest","position":1}]}