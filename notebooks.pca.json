{"version":2,"kind":"Notebook","sha256":"af62dcd34db5d7bce14b7c93759e3da8b06dadabffe5fea45e0b2c71c41e3ec4","slug":"notebooks.pca","location":"/notebooks/PCA.ipynb","dependencies":[],"frontmatter":{"title":"Principal Component Analysis","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"authors":[{"nameParsed":{"literal":"Jacob Tindan","given":"Jacob","family":"Tindan"},"name":"Jacob Tindan","id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Pooja Hari Ambrish","given":"Pooja Hari","family":"Ambrish"},"name":"Pooja Hari Ambrish","id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Altug Karakurt","given":"Altug","family":"Karakurt"},"name":"Altug Karakurt","id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Ali Fallah","given":"Ali","family":"Fallah"},"name":"Ali Fallah","id":"contributors-myst-generated-uid-3"}],"open_access":true,"license":{"content":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true},"code":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true}},"github":"https://github.com/projectpythia/dust-cookbook","copyright":"2024","numbering":{"title":{"offset":1}},"edit_url":"https://github.com/projectpythia/dust-cookbook/blob/main/notebooks/PCA.ipynb","exports":[{"format":"ipynb","filename":"PCA.ipynb","url":"/dust-cookbook/build/PCA-887e6de6fe8ae741ebf7d47855853c5d.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.The idea of PCA is to reduce the number of variables of a data set, while preserving as much information as possible.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"otS6VbDXKI"}],"key":"th0PocF2Vt"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"In this cookbook we will be implementing PCA to reduce the number of variables in our data set by preserving 95% of the data’s variance","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"i2samA09hi"}],"key":"FjzEDCifVh"}],"key":"XdMf7R4hYe"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# load the required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.preprocessing import scale\nfrom sklearn import preprocessing \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n","key":"qhMAoDPgz7"},{"type":"output","id":"F0xZahrXc8UEyQ9lcM9qM","data":[],"key":"ozUUFrH64I"}],"key":"tYB9mDQcW8"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"The first step is to read the data.\nPlease refer to the Random Forest Regression Cookbook for a detailed explanation about the feature engineering. Here we will be reading the dataset with the new variables obtained after feature engineering.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OtjRBu1FjR"}],"key":"kAP3KTvhL2"}],"key":"TQNtp13oPW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"\ndust_df = pd.read_csv('../saharan_dust_met_cat_vars.csv', index_col='time')\n","key":"oVGDwtE1dr"},{"type":"output","id":"CZJrH9nQCJOCdkLuPIJEi","data":[],"key":"EKbiPsWTFm"}],"key":"cciXueEK0U"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"dust_df","key":"f3SHP9YSmW"},{"type":"output","id":"xA4oDXMrTPLhxVvqbkbP9","data":[{"output_type":"execute_result","execution_count":3,"metadata":{},"data":{"text/plain":{"content":"                 PM10         T2        rh2         slp       PBLH  \\\ntime                                                                 \n1960-01-01  2000.1490  288.24875  32.923786  1018.89420  484.91812   \n1960-01-02  4686.5370  288.88450  30.528862  1017.26575  601.58310   \n1960-01-03  5847.7515  290.97128  26.504536  1015.83514  582.38540   \n1960-01-04  5252.0586  292.20060  30.678936  1013.92230  555.11860   \n1960-01-05  3379.3190  293.06076  27.790462  1011.94934  394.95440   \n...               ...        ...        ...         ...        ...   \n2010-12-27  2681.4685  292.38474  18.858383  1011.69574  315.81320   \n2010-12-28  1345.8488  291.46680  26.357006  1010.66340  232.03355   \n2010-12-29  4500.9810  289.62990  23.169529  1014.53740  557.29913   \n2010-12-30  5150.3840  290.11844  43.158295  1017.36230  745.95575   \n2010-12-31  4753.6760  291.39554  38.372738  1016.40400  697.51697   \n\n            wind_speed_10m  wind_speed_925hPa WIND_DIR  RAIN  \ntime                                                          \n1960-01-01        6.801503          13.483623       NE     0  \n1960-01-02        8.316340          18.027075       NE     0  \n1960-01-03        9.148216          17.995173       NE     0  \n1960-01-04        8.751743          15.806478       NE     0  \n1960-01-05        6.393228           9.160809       NE     0  \n...                    ...                ...      ...   ...  \n2010-12-27        4.749993           7.846004       NE     4  \n2010-12-28        3.051484           3.346668       NE     4  \n2010-12-29        6.249619          13.007574       NE     4  \n2010-12-30        8.769048          19.371056       NE     4  \n2010-12-31        8.137861          17.666280       NE     4  \n\n[18466 rows x 9 columns]","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PM10</th>\n      <th>T2</th>\n      <th>rh2</th>\n      <th>slp</th>\n      <th>PBLH</th>\n      <th>wind_speed_10m</th>\n      <th>wind_speed_925hPa</th>\n      <th>WIND_DIR</th>\n      <th>RAIN</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1960-01-01</th>\n      <td>2000.1490</td>\n      <td>288.24875</td>\n      <td>32.923786</td>\n      <td>1018.89420</td>\n      <td>484.91812</td>\n      <td>6.801503</td>\n      <td>13.483623</td>\n      <td>NE</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-02</th>\n      <td>4686.5370</td>\n      <td>288.88450</td>\n      <td>30.528862</td>\n      <td>1017.26575</td>\n      <td>601.58310</td>\n      <td>8.316340</td>\n      <td>18.027075</td>\n      <td>NE</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-03</th>\n      <td>5847.7515</td>\n      <td>290.97128</td>\n      <td>26.504536</td>\n      <td>1015.83514</td>\n      <td>582.38540</td>\n      <td>9.148216</td>\n      <td>17.995173</td>\n      <td>NE</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-04</th>\n      <td>5252.0586</td>\n      <td>292.20060</td>\n      <td>30.678936</td>\n      <td>1013.92230</td>\n      <td>555.11860</td>\n      <td>8.751743</td>\n      <td>15.806478</td>\n      <td>NE</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-05</th>\n      <td>3379.3190</td>\n      <td>293.06076</td>\n      <td>27.790462</td>\n      <td>1011.94934</td>\n      <td>394.95440</td>\n      <td>6.393228</td>\n      <td>9.160809</td>\n      <td>NE</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2010-12-27</th>\n      <td>2681.4685</td>\n      <td>292.38474</td>\n      <td>18.858383</td>\n      <td>1011.69574</td>\n      <td>315.81320</td>\n      <td>4.749993</td>\n      <td>7.846004</td>\n      <td>NE</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2010-12-28</th>\n      <td>1345.8488</td>\n      <td>291.46680</td>\n      <td>26.357006</td>\n      <td>1010.66340</td>\n      <td>232.03355</td>\n      <td>3.051484</td>\n      <td>3.346668</td>\n      <td>NE</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2010-12-29</th>\n      <td>4500.9810</td>\n      <td>289.62990</td>\n      <td>23.169529</td>\n      <td>1014.53740</td>\n      <td>557.29913</td>\n      <td>6.249619</td>\n      <td>13.007574</td>\n      <td>NE</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2010-12-30</th>\n      <td>5150.3840</td>\n      <td>290.11844</td>\n      <td>43.158295</td>\n      <td>1017.36230</td>\n      <td>745.95575</td>\n      <td>8.769048</td>\n      <td>19.371056</td>\n      <td>NE</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2010-12-31</th>\n      <td>4753.6760</td>\n      <td>291.39554</td>\n      <td>38.372738</td>\n      <td>1016.40400</td>\n      <td>697.51697</td>\n      <td>8.137861</td>\n      <td>17.666280</td>\n      <td>NE</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>18466 rows × 9 columns</p>\n</div>","content_type":"text/html"}}}],"key":"yyA0kPY5Uv"}],"key":"tymcGsV0RH"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"One Hot Encoding","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z6EblFcv5C"}],"identifier":"one-hot-encoding","label":"One Hot Encoding","html_id":"one-hot-encoding","implicit":true,"key":"kVA5y0uJXS"}],"key":"rrQpm4pP0P"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s look at the unique values of the WIND_DIR feature","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dWqLrApEHm"}],"key":"CH5qUDeXEO"}],"key":"kn8coUTW2t"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"dust_df['WIND_DIR'].unique()","key":"gryzWpTOI8"},{"type":"output","id":"voR6ufhwp3Db-LDoedNma","data":[{"output_type":"execute_result","execution_count":4,"metadata":{},"data":{"text/plain":{"content":"array(['NE', 'NW', 'SE', 'SW'], dtype=object)","content_type":"text/plain"}}}],"key":"JFyqXg2U48"}],"key":"BkFyIG7MJ6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Since WIND_DIR is a categorical variable, we have to perform One Hot Encoding(OHE). One Hot Encoding creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature. One Hot Encoding is the process of creating dummy variables.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lcpqkWAxs3"}],"key":"UjCmn3ZHjG"}],"key":"cl3wIoZHU8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"one_hot = pd.get_dummies(dust_df['WIND_DIR'], dtype=int)","key":"NuDX8Odbov"},{"type":"output","id":"UOpuBvhmFTmueqDSTpsbd","data":[],"key":"tIO6Me760Z"}],"key":"Q6oyTojFej"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"one_hot","key":"SKW2QxikKD"},{"type":"output","id":"eCQxoZJVfhJmgrwenGZYg","data":[{"output_type":"execute_result","execution_count":6,"metadata":{},"data":{"text/plain":{"content":"            NE  NW  SE  SW\ntime                      \n1960-01-01   1   0   0   0\n1960-01-02   1   0   0   0\n1960-01-03   1   0   0   0\n1960-01-04   1   0   0   0\n1960-01-05   1   0   0   0\n...         ..  ..  ..  ..\n2010-12-27   1   0   0   0\n2010-12-28   1   0   0   0\n2010-12-29   1   0   0   0\n2010-12-30   1   0   0   0\n2010-12-31   1   0   0   0\n\n[18466 rows x 4 columns]","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NE</th>\n      <th>NW</th>\n      <th>SE</th>\n      <th>SW</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1960-01-01</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-02</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-03</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-04</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-05</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2010-12-27</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-28</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-29</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-30</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-31</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>18466 rows × 4 columns</p>\n</div>","content_type":"text/html"}}}],"key":"YAdQDxUu6d"}],"key":"P2njjqfhPz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"df_new = dust_df.drop('WIND_DIR', axis=1)","key":"QYi2u00Acw"},{"type":"output","id":"KFDdgD0GMj26yGWHlTBKd","data":[],"key":"YpK1pTxvly"}],"key":"Wi7yrUDchu"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s now join the One Hot Encoded Fatures into the original dataframe","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HvbKr5P0CG"}],"key":"LKdDD3N7Bp"}],"key":"zHrpSOBAMS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"df_new = df_new.join(one_hot)","key":"tPxTOHp17Z"},{"type":"output","id":"6KnVIgPfBUtak1uPB_q3O","data":[],"key":"vcCENUyYQo"}],"key":"hCvUi4OxPj"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"df_new","key":"zqTyHVu01p"},{"type":"output","id":"om5gMmDv67oF5FryZconT","data":[{"output_type":"execute_result","execution_count":9,"metadata":{},"data":{"text/plain":{"content":"                 PM10         T2        rh2         slp       PBLH  \\\ntime                                                                 \n1960-01-01  2000.1490  288.24875  32.923786  1018.89420  484.91812   \n1960-01-02  4686.5370  288.88450  30.528862  1017.26575  601.58310   \n1960-01-03  5847.7515  290.97128  26.504536  1015.83514  582.38540   \n1960-01-04  5252.0586  292.20060  30.678936  1013.92230  555.11860   \n1960-01-05  3379.3190  293.06076  27.790462  1011.94934  394.95440   \n...               ...        ...        ...         ...        ...   \n2010-12-27  2681.4685  292.38474  18.858383  1011.69574  315.81320   \n2010-12-28  1345.8488  291.46680  26.357006  1010.66340  232.03355   \n2010-12-29  4500.9810  289.62990  23.169529  1014.53740  557.29913   \n2010-12-30  5150.3840  290.11844  43.158295  1017.36230  745.95575   \n2010-12-31  4753.6760  291.39554  38.372738  1016.40400  697.51697   \n\n            wind_speed_10m  wind_speed_925hPa  RAIN  NE  NW  SE  SW  \ntime                                                                 \n1960-01-01        6.801503          13.483623     0   1   0   0   0  \n1960-01-02        8.316340          18.027075     0   1   0   0   0  \n1960-01-03        9.148216          17.995173     0   1   0   0   0  \n1960-01-04        8.751743          15.806478     0   1   0   0   0  \n1960-01-05        6.393228           9.160809     0   1   0   0   0  \n...                    ...                ...   ...  ..  ..  ..  ..  \n2010-12-27        4.749993           7.846004     4   1   0   0   0  \n2010-12-28        3.051484           3.346668     4   1   0   0   0  \n2010-12-29        6.249619          13.007574     4   1   0   0   0  \n2010-12-30        8.769048          19.371056     4   1   0   0   0  \n2010-12-31        8.137861          17.666280     4   1   0   0   0  \n\n[18466 rows x 12 columns]","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PM10</th>\n      <th>T2</th>\n      <th>rh2</th>\n      <th>slp</th>\n      <th>PBLH</th>\n      <th>wind_speed_10m</th>\n      <th>wind_speed_925hPa</th>\n      <th>RAIN</th>\n      <th>NE</th>\n      <th>NW</th>\n      <th>SE</th>\n      <th>SW</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1960-01-01</th>\n      <td>2000.1490</td>\n      <td>288.24875</td>\n      <td>32.923786</td>\n      <td>1018.89420</td>\n      <td>484.91812</td>\n      <td>6.801503</td>\n      <td>13.483623</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-02</th>\n      <td>4686.5370</td>\n      <td>288.88450</td>\n      <td>30.528862</td>\n      <td>1017.26575</td>\n      <td>601.58310</td>\n      <td>8.316340</td>\n      <td>18.027075</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-03</th>\n      <td>5847.7515</td>\n      <td>290.97128</td>\n      <td>26.504536</td>\n      <td>1015.83514</td>\n      <td>582.38540</td>\n      <td>9.148216</td>\n      <td>17.995173</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-04</th>\n      <td>5252.0586</td>\n      <td>292.20060</td>\n      <td>30.678936</td>\n      <td>1013.92230</td>\n      <td>555.11860</td>\n      <td>8.751743</td>\n      <td>15.806478</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-05</th>\n      <td>3379.3190</td>\n      <td>293.06076</td>\n      <td>27.790462</td>\n      <td>1011.94934</td>\n      <td>394.95440</td>\n      <td>6.393228</td>\n      <td>9.160809</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2010-12-27</th>\n      <td>2681.4685</td>\n      <td>292.38474</td>\n      <td>18.858383</td>\n      <td>1011.69574</td>\n      <td>315.81320</td>\n      <td>4.749993</td>\n      <td>7.846004</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-28</th>\n      <td>1345.8488</td>\n      <td>291.46680</td>\n      <td>26.357006</td>\n      <td>1010.66340</td>\n      <td>232.03355</td>\n      <td>3.051484</td>\n      <td>3.346668</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-29</th>\n      <td>4500.9810</td>\n      <td>289.62990</td>\n      <td>23.169529</td>\n      <td>1014.53740</td>\n      <td>557.29913</td>\n      <td>6.249619</td>\n      <td>13.007574</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-30</th>\n      <td>5150.3840</td>\n      <td>290.11844</td>\n      <td>43.158295</td>\n      <td>1017.36230</td>\n      <td>745.95575</td>\n      <td>8.769048</td>\n      <td>19.371056</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-31</th>\n      <td>4753.6760</td>\n      <td>291.39554</td>\n      <td>38.372738</td>\n      <td>1016.40400</td>\n      <td>697.51697</td>\n      <td>8.137861</td>\n      <td>17.666280</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>18466 rows × 12 columns</p>\n</div>","content_type":"text/html"}}}],"key":"PttBmu34xE"}],"key":"o5aCURljUj"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"X_features = df_new.drop(labels='PM10', axis = 1)","key":"XspprvvAAd"},{"type":"output","id":"NixTXqRpPSjYTeD7yjVNy","data":[],"key":"Xkr1LyujUv"}],"key":"qenksMXllC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"X_features","key":"vtgwHMZrXJ"},{"type":"output","id":"7Ij8nRzSIPyf5yvMg-ACe","data":[{"output_type":"execute_result","execution_count":11,"metadata":{},"data":{"text/plain":{"content":"                   T2        rh2         slp       PBLH  wind_speed_10m  \\\ntime                                                                      \n1960-01-01  288.24875  32.923786  1018.89420  484.91812        6.801503   \n1960-01-02  288.88450  30.528862  1017.26575  601.58310        8.316340   \n1960-01-03  290.97128  26.504536  1015.83514  582.38540        9.148216   \n1960-01-04  292.20060  30.678936  1013.92230  555.11860        8.751743   \n1960-01-05  293.06076  27.790462  1011.94934  394.95440        6.393228   \n...               ...        ...         ...        ...             ...   \n2010-12-27  292.38474  18.858383  1011.69574  315.81320        4.749993   \n2010-12-28  291.46680  26.357006  1010.66340  232.03355        3.051484   \n2010-12-29  289.62990  23.169529  1014.53740  557.29913        6.249619   \n2010-12-30  290.11844  43.158295  1017.36230  745.95575        8.769048   \n2010-12-31  291.39554  38.372738  1016.40400  697.51697        8.137861   \n\n            wind_speed_925hPa  RAIN  NE  NW  SE  SW  \ntime                                                 \n1960-01-01          13.483623     0   1   0   0   0  \n1960-01-02          18.027075     0   1   0   0   0  \n1960-01-03          17.995173     0   1   0   0   0  \n1960-01-04          15.806478     0   1   0   0   0  \n1960-01-05           9.160809     0   1   0   0   0  \n...                       ...   ...  ..  ..  ..  ..  \n2010-12-27           7.846004     4   1   0   0   0  \n2010-12-28           3.346668     4   1   0   0   0  \n2010-12-29          13.007574     4   1   0   0   0  \n2010-12-30          19.371056     4   1   0   0   0  \n2010-12-31          17.666280     4   1   0   0   0  \n\n[18466 rows x 11 columns]","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>T2</th>\n      <th>rh2</th>\n      <th>slp</th>\n      <th>PBLH</th>\n      <th>wind_speed_10m</th>\n      <th>wind_speed_925hPa</th>\n      <th>RAIN</th>\n      <th>NE</th>\n      <th>NW</th>\n      <th>SE</th>\n      <th>SW</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1960-01-01</th>\n      <td>288.24875</td>\n      <td>32.923786</td>\n      <td>1018.89420</td>\n      <td>484.91812</td>\n      <td>6.801503</td>\n      <td>13.483623</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-02</th>\n      <td>288.88450</td>\n      <td>30.528862</td>\n      <td>1017.26575</td>\n      <td>601.58310</td>\n      <td>8.316340</td>\n      <td>18.027075</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-03</th>\n      <td>290.97128</td>\n      <td>26.504536</td>\n      <td>1015.83514</td>\n      <td>582.38540</td>\n      <td>9.148216</td>\n      <td>17.995173</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-04</th>\n      <td>292.20060</td>\n      <td>30.678936</td>\n      <td>1013.92230</td>\n      <td>555.11860</td>\n      <td>8.751743</td>\n      <td>15.806478</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1960-01-05</th>\n      <td>293.06076</td>\n      <td>27.790462</td>\n      <td>1011.94934</td>\n      <td>394.95440</td>\n      <td>6.393228</td>\n      <td>9.160809</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2010-12-27</th>\n      <td>292.38474</td>\n      <td>18.858383</td>\n      <td>1011.69574</td>\n      <td>315.81320</td>\n      <td>4.749993</td>\n      <td>7.846004</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-28</th>\n      <td>291.46680</td>\n      <td>26.357006</td>\n      <td>1010.66340</td>\n      <td>232.03355</td>\n      <td>3.051484</td>\n      <td>3.346668</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-29</th>\n      <td>289.62990</td>\n      <td>23.169529</td>\n      <td>1014.53740</td>\n      <td>557.29913</td>\n      <td>6.249619</td>\n      <td>13.007574</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-30</th>\n      <td>290.11844</td>\n      <td>43.158295</td>\n      <td>1017.36230</td>\n      <td>745.95575</td>\n      <td>8.769048</td>\n      <td>19.371056</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2010-12-31</th>\n      <td>291.39554</td>\n      <td>38.372738</td>\n      <td>1016.40400</td>\n      <td>697.51697</td>\n      <td>8.137861</td>\n      <td>17.666280</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>18466 rows × 11 columns</p>\n</div>","content_type":"text/html"}}}],"key":"nW0Fk8amkD"}],"key":"ZbyYbiibIE"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We will now separate out the features and the target variable","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"afHL3eOorA"}],"key":"yQT23s8oVh"}],"key":"ri0aLtMG75"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"X = df_new.drop(labels='PM10', axis=1).values\ny = df_new['PM10'].values","key":"m1XIiXtFn8"},{"type":"output","id":"ey1vtm2pT7XmyrcaIcTOB","data":[],"key":"Pome8BYWna"}],"key":"MZpGf2rcrA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"X.shape","key":"vfoW0mWmJA"},{"type":"output","id":"S8Iup29W-7YLWlU67xg1S","data":[{"output_type":"execute_result","execution_count":13,"metadata":{},"data":{"text/plain":{"content":"(18466, 11)","content_type":"text/plain"}}}],"key":"hG8XGKeJhg"}],"key":"hNkKj9ikzR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"y.shape","key":"ffJLnzfNvm"},{"type":"output","id":"qjLmx5tczqENaJEv1Mf_u","data":[{"output_type":"execute_result","execution_count":14,"metadata":{},"data":{"text/plain":{"content":"(18466,)","content_type":"text/plain"}}}],"key":"GcYhXMsV55"}],"key":"infSimjm8m"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here we will be splitting the data into training and testing data","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ieNTSTUg17"}],"key":"kSWIPdf11d"}],"key":"bTHTypZ0Ua"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","key":"F42btx9ykU"},{"type":"output","id":"wQ4rJanz7R2zrRj00UvP3","data":[],"key":"dIZNY1yn0t"}],"key":"k9OoTGddaA"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Feature Scaling","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DWN0MnFCNp"}],"identifier":"feature-scaling","label":"Feature Scaling","html_id":"feature-scaling","implicit":true,"key":"KhRcg9XHZP"}],"key":"R8rUgNWHn7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Feature scaling is a vital pre-processing step in machine learning that involves transforming numerical features to a common scale. Scaling techniques aim to normalize the range, distribution, and magnitude of features, reducing potential biases and inconsistencies that may arise from variations in their values.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wtzvgeGGd1"}],"key":"uzHLFdH4Dd"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"The next step is to scale the data. We will be using RobustScaler for scaling.\nRobustScaler is an algorithm that scales features using statistics that are robust to outliers.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"K64NrkRLG9"}],"key":"lFpEQIwWcJ"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile) but can be configured.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"LmLoTUk0rP"}],"key":"GgYS0SKdhy"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"When applying feature scaling, it’s crucial to fit the scaler on the training data and then use the same scaler to transform the test data. This ensures consistency and prevents data leakage.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"rhf2guVsaf"}],"key":"v7gwqhfO25"}],"key":"lCn4Y0lhir"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"scaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","key":"nfpHlkXgKP"},{"type":"output","id":"A3zDUmu9K1EvtFkQ67Ooy","data":[],"key":"U6l94KSulw"}],"key":"Yprpyr7R4m"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We will now use Scikit-Learn’s PCA estimator to find out the number of principal components we need. Since we are not sure how many principal components we need for this data, we will use Minka’s MLE to guess the dimension. Use of n_components == ‘mle’ will interpret svd_solver == ‘auto’ as svd_solver == ‘full’","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uIIAEgy3KD"}],"key":"TaF2b3AfEG"}],"key":"KLRHvHofcV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"pca = PCA(n_components = 'mle', svd_solver = 'full')\n\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n","key":"jGGo2i9Bzz"},{"type":"output","id":"X3K5qDJsCUm3Dmd-H0jrP","data":[],"key":"aR0Wt49fP0"}],"key":"IB08BefKN0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"#Print dataset shape before and after PCA\nprint(f'\\nTraining Data shape: {X_train.shape}')\nprint(f'PCA Training Data shape: {X_train_pca.shape}')","key":"IBBvVvqhTE"},{"type":"output","id":"lHBDZ4avs0aeenKj6aZ5I","data":[{"output_type":"stream","name":"stdout","text":"\nTraining Data shape: (14772, 11)\nPCA Training Data shape: (14772, 10)\n"}],"key":"ngCrUt1QBU"}],"key":"N8jvvT3a3U"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"From the Scikit-learn implementation, we can get the information about the explained variance and plot the cumulative variance.\nThe importance of each component is represented by the explained variance ratio, which indicates the portion of the variance that lies along each principal component.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uvZ7czMZZF"}],"key":"ftgeOdjSJi"}],"key":"WCYdliAena"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"explained_variance = pca.explained_variance_ratio_","key":"hxkM2xyC4r"},{"type":"output","id":"BnQK9s2iHFQxal-cNk-wW","data":[],"key":"ReOjJYzfog"}],"key":"KqbdCVsSyd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"explained_variance","key":"cPNGxn839m"},{"type":"output","id":"MJGyADP0fFeK58TsUSWWk","data":[{"output_type":"execute_result","execution_count":20,"metadata":{},"data":{"text/plain":{"content":"array([0.37721146, 0.25138801, 0.14908775, 0.12167106, 0.04221808,\n       0.01815109, 0.01510624, 0.01231552, 0.00793525, 0.00491554])","content_type":"text/plain"}}}],"key":"iVIUfUZchv"}],"key":"gySZKdfhGr"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"By summing the explained variance ratio of the first ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gePutUo5SW"},{"type":"inlineMath","value":"N{pc}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mrow><mi>p</mi><mi>c</mi></mrow></mrow><annotation encoding=\"application/x-tex\">N{pc}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">c</span></span></span></span></span>","key":"GScGwu6CFH"},{"type":"text","value":" components, we obtain the so-called cumulative explained variance","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dkpFurmG6W"}],"key":"kLZOifRIgF"}],"key":"sGE019aTq4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"cumulative_var_ratio = np.cumsum(explained_variance)","key":"mRgiGWVncX"},{"type":"output","id":"Fgu5rTCaKtVnFzpczNMkm","data":[],"key":"wTikHYROPX"}],"key":"tbyHB7XAHG"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"cumulative_var_ratio","key":"GQ2oAgg6Fp"},{"type":"output","id":"IARwO2JrFF7prirA-rhcd","data":[{"output_type":"execute_result","execution_count":22,"metadata":{},"data":{"text/plain":{"content":"array([0.37721146, 0.62859947, 0.77768722, 0.89935828, 0.94157636,\n       0.95972744, 0.97483369, 0.98714921, 0.99508446, 1.        ])","content_type":"text/plain"}}}],"key":"CPgzIjOdD7"}],"key":"py9XpklBTF"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The minimum number of principal components required to preserve the 95% of the data’s variance can be computed with the following command","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r75udXqU8e"}],"key":"CU6pTG0M2r"}],"key":"YUj9y5WtVM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"d = np.argmax(cumulative_var_ratio >= 0.95) + 1\nd","key":"N703KTkzw4"},{"type":"output","id":"xmw3gnbW5pchx4-udLy1k","data":[{"output_type":"execute_result","execution_count":23,"metadata":{},"data":{"text/plain":{"content":"np.int64(6)","content_type":"text/plain"}}}],"key":"lPmdn5G3S9"}],"key":"kYa8824wRv"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s plot the cumulative variance for the number of principal components","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"C7nRaMZMBH"}],"key":"XNF4eauUye"}],"key":"WAX15mRwqL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.rcParams[\"figure.figsize\"] = (10,6)\nfig, ax = plt.subplots()\nxi = np.arange(0,10, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\nplt.xlim([0,10])\nplt.ylim([0.0,1.1])\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0,10, step=1)) \nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=10)\n\nax.grid(axis='x')\nplt.show()","key":"jTDDG4Gxp0"},{"type":"output","id":"A4YxUpQNlmO8QzZLAB1il","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 1000x600 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"0d1ca27a636fa83caa7b2d7b014057d2","path":"/dust-cookbook/build/0d1ca27a636fa83caa7b2d7b014057d2.png"}}}],"key":"aGUxImYawX"}],"key":"hlzPIyq8xe"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"This plot illustrates how the cumulative variance explained by principal components increases as more components are added. It helps determine the number of components needed to capture the majority of the variance in the dataset.\nHence we can conclude from the plot and from calculation that in order to preserve 95% of the data’s variance, we need 6 principal components.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V4Fh1xvApo"}],"key":"JEFPXaCzme"}],"key":"XMPfYFFCOZ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now let’s run the PCA estimator with n_components set to 6","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CYyhTcer0D"}],"key":"iyK6rCvmGf"}],"key":"ZDmmEXv9Ai"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"pca = PCA(n_components = 6, svd_solver = 'full')\n\nX_train_pca_6 = pca.fit_transform(X_train)\nX_test_pca_6 = pca.transform(X_test)\n","key":"Q2Y4OHH1Pb"},{"type":"output","id":"3RHeVJ20BKEEXdWzT2y28","data":[],"key":"UrxJxFOmwg"}],"key":"Fm8707WPjF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(pca.components_)","key":"utn30mGMAa"},{"type":"output","id":"eyljWIWU7BZkphuOtu8je","data":[{"output_type":"stream","name":"stdout","text":"[[-3.61778391e-01 -3.38369866e-01  4.51569378e-01 -1.77126326e-01\n   4.96939306e-01  4.18413802e-01 -1.24063367e-01  2.35693347e-01\n  -5.11995608e-02 -3.42324113e-02 -1.50261375e-01]\n [-1.62573966e-01  6.98539654e-01  3.53914136e-01  4.49498676e-01\n   2.20612234e-01  1.69030327e-01  1.78128216e-01 -1.60511838e-01\n  -7.38533169e-03  1.53053509e-02  1.52591819e-01]\n [ 3.29626953e-01 -2.98494821e-01 -2.85023040e-01  6.97675877e-01\n   2.57328897e-01  2.60838223e-01 -3.15504826e-01 -1.39799500e-02\n  -1.47564894e-03 -7.00179657e-04  1.61557786e-02]\n [ 1.65111754e-01 -2.63244559e-01 -7.53441771e-02  1.07850312e-01\n   1.06558189e-01  1.34670039e-01  9.20119383e-01  8.20988846e-02\n  -1.97557125e-02 -9.40257531e-03 -5.29405967e-02]\n [-1.43147624e-01 -3.46012782e-01  5.56860207e-01  4.37957853e-01\n  -4.84413002e-01 -2.75072184e-01  1.18798913e-02 -8.92497782e-03\n   1.35216504e-01  3.95280255e-02 -1.65819552e-01]\n [ 9.90391435e-02  2.46357985e-01 -2.31771115e-02  1.33495943e-01\n  -1.09713471e-01 -1.00560474e-01 -3.16829952e-02  7.63185470e-01\n  -4.79819794e-01 -9.09281230e-03 -2.74272863e-01]]\n"}],"key":"ijsLc8nNeT"}],"key":"pyKW5LFy9Z"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"#Print dataset shape before and after PCA\nprint(f'\\nTraining Data shape: {X_train.shape}')\nprint(f'PCA Training Data shape: {X_train_pca_6.shape}')","key":"M1ATqsdeE1"},{"type":"output","id":"pAHuI09fBrQDpo80ZUpcc","data":[{"output_type":"stream","name":"stdout","text":"\nTraining Data shape: (14772, 11)\nPCA Training Data shape: (14772, 6)\n"}],"key":"P3YLrILxj7"}],"key":"dhB2ykXmB3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# number of components\nn_pcs= pca.components_.shape[0]\nn_pcs","key":"JhCkCpAp2M"},{"type":"output","id":"Gnhx4hBnVhEyE74mkazqQ","data":[{"output_type":"execute_result","execution_count":28,"metadata":{},"data":{"text/plain":{"content":"6","content_type":"text/plain"}}}],"key":"wL9BmCTz9h"}],"key":"bnWFaRECE8"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We now know that out of 11 features, 6 features were selected by PCA as relevant features but we do not know what those 6 features are. We can do that by first finding the index of the feature with the largest absolute value in each principal component’s loading vector using list comprehension. Then we can map these indices to their corresponding feature names from the original dataset. The next step is to create a dictionary that associates each principal component with its most important feature and convert the dictionary to a dataframe","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mYajtxk36W"}],"key":"igVsyqvxP9"}],"key":"rJQ0lCBjeQ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# get the index of the most important feature on EACH component(largest absolute value)\n\nmost_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]","key":"mOvWF488e6"},{"type":"output","id":"qzZabDEDqSDnZ1DvUqksN","data":[],"key":"ErRA2ohwtg"}],"key":"s3q75Rgul4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"#get the corresponding feature names from the original dataset\ninitial_feature_names = list(X_features.columns.values)\ninitial_feature_names","key":"CGV0T6pcBC"},{"type":"output","id":"IGlGlJoErNWUHbUET0faE","data":[{"output_type":"execute_result","execution_count":30,"metadata":{},"data":{"text/plain":{"content":"['T2',\n 'rh2',\n 'slp',\n 'PBLH',\n 'wind_speed_10m',\n 'wind_speed_925hPa',\n 'RAIN',\n 'NE',\n 'NW',\n 'SE',\n 'SW']","content_type":"text/plain"}}}],"key":"Al8v9L8QSD"}],"key":"fpprifa9zj"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# map the indices to their corresponding feature names from the original dataset\nmost_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]","key":"Yrrcf4nl35"},{"type":"output","id":"7Erx306BWalp3q7APz_KU","data":[],"key":"m9ibI0Y8P2"}],"key":"l15vX0sESH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# create a dictionary that associates each principal component with its most important feature \ndic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}","key":"sTX0hIiOMx"},{"type":"output","id":"mllzQ3xnTb9tGewohRyUr","data":[],"key":"WuQi8FTNuO"}],"key":"uubTiBUnV0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# build the dataframe\ndf = pd.DataFrame(sorted(dic.items()))","key":"aHbpTV6QEu"},{"type":"output","id":"q1Z4eecROGaSFysZvS-3a","data":[],"key":"IKjyvv0W7e"}],"key":"tSWzeds4ci"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"df","key":"yD5sL0ijgU"},{"type":"output","id":"QeHLTOEx350N06RGhjelw","data":[{"output_type":"execute_result","execution_count":34,"metadata":{},"data":{"text/plain":{"content":"     0               1\n0  PC1  wind_speed_10m\n1  PC2             rh2\n2  PC3            PBLH\n3  PC4            RAIN\n4  PC5             slp\n5  PC6              NE","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PC1</td>\n      <td>wind_speed_10m</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PC2</td>\n      <td>rh2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PC3</td>\n      <td>PBLH</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PC4</td>\n      <td>RAIN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PC5</td>\n      <td>slp</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>PC6</td>\n      <td>NE</td>\n    </tr>\n  </tbody>\n</table>\n</div>","content_type":"text/html"}}}],"key":"XKGdW3QA2M"}],"key":"lOnAzWoOKW"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Now we know the 6 principal components that were selected by the PCA estimator.\nThey are","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Jc68Y4EcuD"}],"key":"BRfFezK7Ef"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"wind_speed_10m","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"R66rYVJLl9"}],"key":"KYWRKETZrI"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"rh2","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"ZEMTUr2vX9"}],"key":"Tncu7f6ZYl"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"PBLH","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"JncQQtvhdy"}],"key":"YhSUDr5SO3"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"RAIN","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"RcMPHPWQlj"}],"key":"l51kRdJHby"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"slp","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"DXYLqgGTkI"}],"key":"ZEPCFtdTX0"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"NE","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"P02Z90FhJX"}],"key":"qz3LI7EjgN"}],"key":"zO0Mlunpad"}],"key":"ovATLBxIW0"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s visualize our results","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZsGohHNMGN"}],"identifier":"lets-visualize-our-results","label":"Let’s visualize our results","html_id":"lets-visualize-our-results","implicit":true,"key":"sstKPU60xx"}],"key":"COLaG0C00U"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The PCA-transformed data is clustered using KMeans with 6 clusters.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b5lHFUGCxl"}],"key":"GbHA3UWU0C"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"K-means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into K clusters. It works by iteratively assigning each data point to the nearest centroid and then updating the centroids based on the mean of the data points assigned to each cluster.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"UqH7wpGy0j"}],"key":"nQgcVs2G83"}],"key":"QVGUGIuKFW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"\n\n# Cluster the PCA-transformed data\nkmeans = KMeans(n_clusters=6, random_state=42)\ncluster_labels = kmeans.fit_predict(X_train_pca_6)\n\n# Create a DataFrame combining PCA components and cluster labels\npca_cluster_df = pd.DataFrame(X_train_pca_6, columns=[f'PC{i+1}' for i in range(6)])\npca_cluster_df['Cluster'] = cluster_labels\n\n# Get the feature names associated with each cluster\nn_pcs = pca.components_.shape[0]\nmost_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\ninitial_feature_names = list(X_features.columns.values)\nmost_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n\n# Plot the scatter plot\nplt.figure(figsize=(10, 8))\nsns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_cluster_df, palette='tab10')\nplt.title('PCA Components in Clusters')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\n\n# Create custom legend\nlegend_labels = {i: most_important_names[i] for i in range(len(most_important_names))}\nplt.legend(title='Feature', labels=legend_labels.values())\n\nplt.show()\n","key":"CZZ2LcIYLp"},{"type":"output","id":"kVPw9uifClyT8UQz5KvA-","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 1000x800 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"7d6f0e328d6c03004520044e4999b858","path":"/dust-cookbook/build/7d6f0e328d6c03004520044e4999b858.png"}}}],"key":"GByya0bafr"}],"key":"WeiiB386wL"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here the PCA components and cluster labels are combined into a DataFrame. A scatter plot is created where each point represents a data sample, and the points are colored based on their cluster assignment.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s6m29CVk7l"}],"key":"baKYNPbHnX"}],"key":"CiqmiP1jGt"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"In the upcoming notebook, we’ll delve into Self-Organizing Maps (SOM) and explore their application on this dataset. We will discover how SOMs are leveraged for clustering and visualization, offering insights into the underlying structures and patterns within the data.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JBgvUi4Zwa"}],"key":"ZV746s8DTs"}],"key":"Vu6zxq7t8z"}],"key":"Ggnn4Zzn7d"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Exploratory Data Analysis","url":"/notebooks/data-exploration","group":"Data Exploration"},"next":{"title":"Self-organizing Maps (SOM)","url":"/notebooks/som","group":"Self Organizing Maps"}}},"domain":"http://localhost:3000"}