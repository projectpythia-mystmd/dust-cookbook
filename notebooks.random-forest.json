{"version":2,"kind":"Notebook","sha256":"f2bc7d2ff2ac5b608d231807a9d663d1188a9e271ec9c1a8033f024ed03bfb90","slug":"notebooks.random-forest","location":"/notebooks/random_forest.ipynb","dependencies":[],"frontmatter":{"title":"Random Forest Regression","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"authors":[{"nameParsed":{"literal":"Jacob Tindan","given":"Jacob","family":"Tindan"},"name":"Jacob Tindan","id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Pooja Hari Ambrish","given":"Pooja Hari","family":"Ambrish"},"name":"Pooja Hari Ambrish","id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Altug Karakurt","given":"Altug","family":"Karakurt"},"name":"Altug Karakurt","id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Ali Fallah","given":"Ali","family":"Fallah"},"name":"Ali Fallah","id":"contributors-myst-generated-uid-3"}],"open_access":true,"license":{"content":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true},"code":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true}},"github":"https://github.com/projectpythia/dust-cookbook","copyright":"2024","numbering":{"title":{"offset":1}},"edit_url":"https://github.com/projectpythia/dust-cookbook/blob/main/notebooks/random_forest.ipynb","exports":[{"format":"ipynb","filename":"random_forest.ipynb","url":"/dust-cookbook/build/random_forest-d621cd66bed1371f22d5ce1623727fe2.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"In this exercise, we are using machine learning as a tool for data exploration. We are interested in discovering the impact of the other features (attributes) on PM10 dust concentration. One way to investigate this is to implement a PM10 predictor that uses the other features as input and then check the contribution of each attribute to the learning algorithm. Our algorithm of chocie is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KUSUZIUHbV"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"random forests","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rT5RAVAhDg"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html","key":"TPWW8dqVRk"},{"type":"text","value":" due to the interpretability of the outcome.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NjxnC4418X"}],"key":"RLvu1eWN9F"}],"key":"sghmsU90O1"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Random forest","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W5wNs4QwMA"}],"key":"KUNbfR6Oep"},{"type":"text","value":" is an ensemble algorithm, a class of learning algorithms that rely on combining the predictions of many ‘weak’ learners instead of relying on a single ‘strong’ learner. In practice, this approach is used to avoid the common issue of overfitting, where a learning algorithm fits the given dataset too well. In short, an overfit algorithm goes beyond just learning the nature and statistical properties data, it learns the particular intricacies of the specific examples in the dataset. Hence, it deviates from learning the trends in the data and learning the irrelevant noise in it. In other words, an overfit algorithm loses sight of the forest for the trees. The aptly named random forest algorithm instead uses an ensemble of learners, called decision trees, that are trained to lean towards under-fitting rather than over-fitting and uses the consensus of these weak learners to come to a final decision.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PBJk0RUKkm"}],"key":"c42VksXk59"}],"key":"otoSkH7sfa"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uBHkotcXbI"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"decision trees","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"T4s7Zuigh6"}],"key":"hxpaCuINHB"},{"type":"text","value":" that make up the random forest abstract the idea of a flowchart, mimicking a natural way humans make decisions. The training of a decision tree starts at the root node and at every step asks the question, “What is the most distinctive feature and how does it distinguish the target variable?”. Based on the answer, the tree branches and keeps iteratively asking the same question at each branch for the subset of data that lands on the given side. Thinking back to out primary goal with this cookbook, the decision trees are internally asking the same questions that we do.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A8Pu07W8cN"}],"key":"z4s9nhQ6dk"}],"key":"hMgNYBImMx"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor ","key":"u2cUEpwx61"},{"type":"output","id":"ldHVhi-yuJdKYMjoeTKld","data":[],"key":"xJvDNZPFFG"}],"key":"kIkRvHOMYn"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# We are loading the edited version of the dataset we saved in the first section.\ndust_df = pd.read_csv('../saharan_dust_met_cat_vars.csv', index_col='time')\n\n# Similar to the PCA example, we need to one-hot encode the wind direction since random forests only support numerical features.\none_hot = pd.get_dummies(dust_df['WIND_DIR'])\ndust_df = dust_df.drop('WIND_DIR', axis=1)\ndust_df = dust_df.join(one_hot)\n\n# Separating our data into the target variable (PM10) and the features (the rest fo the attributes)\ny = dust_df['PM10'].values\nX = dust_df.drop(['PM10'], axis=1).values","key":"NwkMwmLQPN"},{"type":"output","id":"C0bMTHYrWpxDp0-9UnNZQ","data":[],"key":"ZFgK15uWCc"}],"key":"JCFzM2IVAP"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"A key step in training learning algorithms is the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lSZchF0BkW"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"hyperparameter tuning","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Dc9kNpo7Qa"}],"key":"UKBE3eeqag"},{"type":"text","value":" step. These parameters specify the predictor we are using and can be used as levers to fine tune how well the algorithm fits the given dataset. These chocies are critical in preventing under-fitting and over-fitting in most learning algorithms. However, due to our choice of a resilient ensemble algorithm, these choices end up being less impactful than other algorithms like neural networks or a stand-alone decision-tree. We include the best performing choices of the parameters in our attempts, but feel free to change these values to see how they imapct the rest of our analysis.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WqJt3Yv6sm"}],"key":"lCtZ1AX5Ii"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Below is a quick summary of how the most significant hyper-parameters impact the resultant random forest. For much more detail, you can find the in-depth ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"maspIlBTr8"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"user guide","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"SyJJkERJS8"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html","key":"pcAfYTtwu4"},{"type":"text","value":".","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"JCVtMtiejy"}],"key":"HcETkLwFM8"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"inlineCode","value":"n_estimators","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"wJJsQtlc5A"},{"type":"text","value":": This is the number of decision-trees included in the random forest. The larger the number, the more robust we expect the algorithm to get to overfitting, which comes at a computational cost during training.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"DcgKUROUDU"}],"key":"lfwgzVKzVE"}],"key":"L2Ze8ZmGJF"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"inlineCode","value":"max_depth","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"JRGh8gbRAg"},{"type":"text","value":": The maximum depth for the decision-trees in the algorithm. The deeper the tree gets, the more likely it is to over-fit (and conversely, the shallower the tree, the more likely it is to under-fit). Using an ensemble of trees instead of a single one reduces the impact of this hyper-parameter.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"wOQZnXQtkd"}],"key":"ljKE1Lkd2p"}],"key":"jRHR3H0Ian"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"inlineCode","value":"max_features","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"QjtwYeTDn9"},{"type":"text","value":": The number of features each branch can make decisions on. The larger the number, the likelier a tree is to over-fit and vice versa. Similar to ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"Ay5NV8vC0A"},{"type":"inlineCode","value":"max_depth","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"ogUkeMoICD"},{"type":"text","value":", using random forest makes it less crucial to tune this parameter well. We leave it at its default value.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"lKsUDXzxLJ"}],"key":"AzPdx5puV9"}],"key":"DStWmggDUk"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"inlineCode","value":"n_jobs","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"ktkgsaErFX"},{"type":"text","value":": Enables training decision-trees in parallel. Our choices maximizes the parallelisim to complete training as fast as possible.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"OgJC0Ym4uz"}],"key":"kYIEddT8Md"}],"key":"dv6ggTFGUq"}],"key":"sTc0fAjaE1"}],"key":"GKkqON3xMu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"rf = RandomForestRegressor(n_estimators=100, max_depth=100, n_jobs=-1)\n\n# This step might take a few minutes. We slice the data into 5 random chunks.\n# Then, each chunk is spared for testing, the training is done on the other 4 and tested on the spared chunk.\n# This is repeated 5 times and the average of the result is reported. \nscores = cross_val_score(rf, X, y, cv=5, scoring=\"neg_mean_absolute_percentage_error\")\nprint(f\"The random forest predicts the dust level with {scores.mean()+1} average accuracy\")","key":"eA5uy8HYHH"},{"type":"output","id":"Ipc75j0sCS3gh_-XoOiig","data":[{"output_type":"stream","name":"stdout","text":"The random forest predicts the dust level with 0.6082033794107047 average accuracy\n"}],"key":"GkKfHvrmQY"}],"key":"BAWnrGiGPX"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"For evaluation, we are using a relative metric. The reported loss value is the relative difference (w.r.t to the true value) between the true and the predicted value. So, the ~60% accuracy we get here means our prediction deviates from the true value by 40% on average. Although this figure looks large at first glance, notice that the target value varies across a large range that makes orders of magnitude mistakes possible.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OFxPDwgw5t"}],"key":"W0N9C6kMDU"}],"key":"QmMWWNBI21"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"dust_df['PM10'].hist(bins=25);","key":"NKZ41NF37Y"},{"type":"output","id":"fBNMKbmf3d5nlPeDf6yBv","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"ca4491300b1b2e94fa42380288560a34","path":"/dust-cookbook/build/ca4491300b1b2e94fa42380288560a34.png"}}}],"key":"gdhxZAvu8r"}],"key":"PUzavX3k37"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This step is a typical part of the machine learning workflow for tuning hyper-parameters. Once we finalize the hyper-parameters, we can re-train the algorithm on all of our data (instead of 4/5 of it) and start exploring te importance of the features on our target variable.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bZghnJMGA4"}],"key":"rM7lBLh5KF"}],"key":"mbomsFWxV7"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Training a new random forest on our entire data\ny = dust_df['PM10'].values\nX = dust_df.drop(['PM10', 'wind_speed_10m'], axis=1).values\nfeatures = dust_df.drop(['PM10'], axis=1).columns\nrf = RandomForestRegressor(n_estimators=100, max_depth=100, n_jobs=-1)\nrf.fit(X, y);","key":"SChQe4NXic"},{"type":"output","id":"Gdap7I-QDXNeVjTBgvL9j","data":[],"key":"k8Yh3BxtIy"}],"key":"A3XvNyTyWz"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"A very useful property of random forests is their interpretability. As useful as they are in practice, quite a lot of learning algorithms are hard to interpret once they are trained. Although there are exceptions, artificial neural networks are hard to make sense of despite their impressive performance, making them practical black-boxes. Random forests on the other hand, keep track of the importance of each feature and the ranges of each that dictate the target variable. We use this information to rank how our features impact the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o9e7amIPiH"},{"type":"inlineCode","value":"PM10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cRWHwnRpyH"},{"type":"text","value":" dust levels.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GwKKSe6T3j"}],"key":"bBR1je4Q1h"}],"key":"kx94XHyjWl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"fig, ax = plt.subplots(1,1, figsize=(22,14), sharex=False, sharey=False,\n                               constrained_layout=True)\nsorted_idx = rf.feature_importances_.argsort()\nimportances = rf.feature_importances_\n\n# Make a bar chart\ncols = np.array(['r','b','g','c','k','pink','purple','magenta','olive'])\nax.set_xlabel('Random Forest Feature Importance')\nax.set_ylabel('Features')\nax.set_axisbelow(True)\nax.grid()\nax.barh(features[sorted_idx],\n           importances[sorted_idx], color=cols);","key":"vk0wb6XEk4"},{"type":"output","id":"Fij-DEOsqBuoezHBBKF81","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 2200x1400 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"62553dc88273119429f53343739da61d","path":"/dust-cookbook/build/62553dc88273119429f53343739da61d.png"}}}],"key":"DV7YckXegL"}],"key":"j2phziWx5J"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We see in this figure the significant impact of the wind speed at 10 meters. However, this dominance overshadows the impact of the other featsures. To see how important each feature is, we use the accuracy of our predictor that has all the features as the baseline and re-train predictors by ;eaving one feature out at a time. Then, comparing the performance of these predictors with the benchmark can help us quantify how much our predictor’s performance changes by including each feature.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VXGe9th3zB"}],"key":"OSBrMFPNJk"}],"key":"YHjtk39U5l"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"benchmark_accuracy = scores.mean()\nleave_one_out_accuracies = []\n\n# We exclude the one-hot encoded features\nleave_one_out_features = ['T2', 'rh2', 'slp', 'PBLH', 'wind_speed_10m', 'wind_speed_925hPa','RAIN']\n\nfor feat in leave_one_out_features:\n    dropped_labels = ['PM10']\n    dropped_labels.append(feat)\n    features = dust_df.drop(dropped_labels, axis=1)\n    X = dust_df.drop(dropped_labels, axis=1).values\n    rf = RandomForestRegressor(n_estimators=100, max_depth=100, n_jobs=-1);\n    current_scores = cross_val_score(rf, X, y, cv=5, scoring=\"neg_mean_absolute_percentage_error\")\n    leave_one_out_accuracies.append(current_scores.mean())\n    print(f\"The random forest predicts the dust level with {current_scores.mean()+1} accuracy when we leave {dropped_labels[1]} out\")","key":"SVzCqfMAby"},{"type":"output","id":"OX6X0NqQlwUHnlQIig2yL","data":[{"output_type":"stream","name":"stdout","text":"The random forest predicts the dust level with 0.5935199388272021 accuracy when we leave T2 out\n"},{"output_type":"stream","name":"stdout","text":"The random forest predicts the dust level with 0.582325927370131 accuracy when we leave rh2 out\n"},{"output_type":"stream","name":"stdout","text":"The random forest predicts the dust level with 0.6376632585583049 accuracy when we leave slp out\n"},{"output_type":"stream","name":"stdout","text":"The random forest predicts the dust level with 0.586702243785429 accuracy when we leave PBLH out\n"},{"output_type":"stream","name":"stdout","text":"The random forest predicts the dust level with 0.503181180615582 accuracy when we leave wind_speed_10m out\n"},{"output_type":"stream","name":"stdout","text":"The random forest predicts the dust level with 0.597948966493362 accuracy when we leave wind_speed_925hPa out\n"},{"output_type":"stream","name":"stdout","text":"The random forest predicts the dust level with 0.6029588023687367 accuracy when we leave RAIN out\n"}],"key":"kFYBAvX1p7"}],"key":"zzk3HPWNCh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"fig, ax = plt.subplots(1,1, figsize=(22,14), sharex=False, sharey=False, constrained_layout=True);\nax.barh(leave_one_out_features, [benchmark_accuracy-loo for loo in leave_one_out_accuracies]);\nax.set_xlabel('Change in prediction accuracy when each feature is added to the dataset');\nax.set_ylabel('Features');","key":"qkSBmkERqy"},{"type":"output","id":"33_Zcx1K5oJL7sttV7b2Z","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 2200x1400 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"68c4b9a345fa276715e43037222a86d5","path":"/dust-cookbook/build/68c4b9a345fa276715e43037222a86d5.png"}}}],"key":"CIAtlIlbLE"}],"key":"b1q5icTEe5"}],"key":"jsVlZWCod5"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Self-organizing Maps (SOM)","url":"/notebooks/som","group":"Self Organizing Maps"}}},"domain":"http://localhost:3000"}